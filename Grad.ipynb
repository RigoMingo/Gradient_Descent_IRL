{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q learning initializing constants\n",
    "\n",
    "# DeltaT, it will be used to turn the infinite time system into a discrete time\n",
    "# system and give it steps for ease of algorithms: k, k+1, k+2\n",
    "\n",
    "deltaT = 0.01\n",
    "\n",
    "# Time that will be quantized by the quatization constant,\n",
    "# used for online evaluation of learned network\n",
    "\n",
    "maxQuantizedTime = int(20/deltaT)\n",
    "\n",
    "# State and Control Effort penalizing matrices, typically constants, deploid in\n",
    "# the local cost function. Tells the algorithm how to go about controlling the\n",
    "# states/control input.\n",
    "\n",
    "#Q = np.array([[1e5, 0, 0], [0, 1e5, 0], [0, 0, 1e5]])\n",
    "#R = np.array([[1e3, 0, 0], [0, 1e3, 0], [0, 0, 1e3]])*0.5\n",
    "\n",
    "# Q = deltaT * Q\n",
    "# R = deltaT * R\n",
    "\n",
    "# numberOfStates = Q.shape[0]\n",
    "# numberOfControls = R.shape[1]\n",
    "\n",
    "numberOfStates = 2\n",
    "numberOfControls = 1\n",
    "\n",
    "# Intsead of inputing a single sample to the system and looping it to make it\n",
    "# learn, we add more patterns to make it learn faster so it learns how\n",
    "# different samples react and adjust learning accordingly\n",
    "\n",
    "numberOfPatterns = 1000\n",
    "\n",
    "# NN training epochs\n",
    "numberOfEpochs = 100\n",
    "\n",
    "# Maximum magnitude of the states and control input\n",
    "\n",
    "stateDomain = 1\n",
    "controlDomain = 30\n",
    "\n",
    "# Discount Factor, how much will the future cost impact impact the\n",
    "# instantaneous cost\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# Max number of times the loop will repeat,\n",
    "# could be less if diverged/error is small enough\n",
    "\n",
    "maxLooping = 200\n",
    "\n",
    "# Basis functions, a polynomial used, along with the weights, for\n",
    "# function approximation\n",
    "\n",
    "def phi(x1, x2, u):\n",
    "    \n",
    "    # Change dimension when changing the amount of basis functions\n",
    "    # Type is object to apease sympy, if used for calculation purposes\n",
    "    # use \".astype(float)\" to change format\n",
    "    \n",
    "    out = np.zeros((17, np.size(x1)), dtype=object)\n",
    "\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    u = np.array(u)\n",
    "    \n",
    "    out[0,:] = 1\n",
    "    \n",
    "    out[1,:] = x1\n",
    "    out[2,:] = x2\n",
    "    out[3,:] = u\n",
    "    \n",
    "    out[4,:] = x1**2\n",
    "    out[5,:] = x2**2\n",
    "    out[6,:] = u**2\n",
    "    out[7,:] = x1 * x2\n",
    "    out[8,:] = x1 * u\n",
    "    out[9,:] = x2 * u\n",
    "    \n",
    "    out[10,:] = x1**3 \n",
    "    out[11,:] = x2**3 \n",
    "    # out[12,:] = x1**2 * x2 \n",
    "    out[12,:] = x1**2 * u \n",
    "    out[13,:] = x2**2 * u\n",
    "    # out[15,:] = x2**2 * x1 \n",
    "    out[14,:] = x1 * x2 * u  \n",
    "    out[15,:] = u**2 * x1 \n",
    "    out[16,:] = u**2 * x2 \n",
    "    # out[19,:] = u**3 \n",
    "    \n",
    "    # u cannot be cubic as the derivative would be squared,\n",
    "    #solution could have imaginary component\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# The drift dynamics of the system have been discretized below, F & G\n",
    "\n",
    "def F(x):\n",
    "    x1 = np.array(x[0,:])\n",
    "    x2 = np.array(x[1,:])\n",
    "\n",
    "    # outF = np.zeros((numberOfStates, np.size(x1)))\n",
    "    f = np.array([\n",
    "        x2, \n",
    "        ( (1-x1**2) * x2 - x1) \n",
    "    ])\n",
    "\n",
    "    return x + deltaT*f\n",
    "    \n",
    "G = deltaT * np.array([[0], [1]])\n",
    "\n",
    "def cs(x):\n",
    "    return x.reshape(x.shape[0], 1)\n",
    "\n",
    "# This are the coefficients for the basis functions used in function \n",
    "# approximation\n",
    "\n",
    "numberOfNeurons = np.size(phi(1,1,1))\n",
    "weights_LIP = np.random.randn(numberOfNeurons, 1)\n",
    "\n",
    "# Random States and Controls used for training in their respective domain\n",
    "\n",
    "xPatterns = stateDomain * (2 * np.random.rand(numberOfStates,numberOfPatterns) - 1)\n",
    "uPatterns = controlDomain * (2 * np.random.rand(numberOfControls, numberOfPatterns) - 1)\n",
    "\n",
    "# the G dynamics using xPatterns as inputs\n",
    "uIn = G\n",
    "\n",
    "# Derivative of the basis function with respect to u, used to update the policy\n",
    "# and is solved through symbolically.\n",
    "\n",
    "x1,x2,u = sp.symbols('x1,x2,u')\n",
    "basisPhi = sp.Matrix(phi(x1,x2,u))\n",
    "derivePhi = sp.diff(basisPhi, u)\n",
    "\n",
    "# PHI is the value the basis functions output when xPatterns and \n",
    "# uPatterns are used as input\n",
    "\n",
    "PHI = phi(xPatterns[0,:], xPatterns[1,:], uPatterns[0,:]).astype(float)\n",
    "\n",
    "weightHistory = np.zeros((numberOfNeurons, maxLooping))\n",
    "\n",
    "ERROR = np.zeros((1, maxLooping))\n",
    "\n",
    "# Training a NN on tensorflow requires an optimizer (way of learning; adam is modified gradient descent) and a metric (how does it know its learning correctly)\n",
    "optimizer = Adam()\n",
    "acc_metric = MeanSquaredError()\n",
    "loss_train = tf.keras.losses.Huber(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning Loop\n",
    "def Q_Learn(R, Q, weights_LIP):\n",
    "    for k in range(maxLooping):\n",
    "        \n",
    "        # uIn refers to the initial control, uIn = G(xPatterns)\n",
    "        xNext = F(xPatterns) + G * uPatterns\n",
    "        \n",
    "        weightSym = sp.Matrix(weights_LIP)\n",
    "        derEqual0 = weightSym.dot(derivePhi)\n",
    "        solutionU = sp.solve(derEqual0, u)\n",
    "        uNextFunction = sp.lambdify([x1,x2], solutionU, 'numpy')\n",
    "\n",
    "        # The xNext is obtained from training in the simulation space, could be obtained by dynamics\n",
    "        uNext = np.array(uNextFunction(xNext[0,:], xNext[1,:]))\n",
    "        \n",
    "        futureCost = (weights_LIP.T @ phi(xNext[0,:], xNext[1,:], uNext).astype(float)).reshape(1, numberOfPatterns)\n",
    "        \n",
    "        cost = (0.5 * np.sum((Q @ xPatterns**2 ), axis = 0)\n",
    "                + 0.5 * np.sum((R @ uPatterns**2), axis = 0) + gamma * futureCost)\n",
    "\n",
    "        oldWeights = weights_LIP\n",
    "        weightHistory[:, k] = oldWeights.reshape(numberOfNeurons,)\n",
    "        \n",
    "        # Least squares method to find the next set of weights, Ax = B\n",
    "        \n",
    "        A = PHI @ PHI.T\n",
    "        B = PHI @ cost.T\n",
    "        weights_LIP = np.linalg.solve(A, B)\n",
    "        \n",
    "        ERROR[:, k] = np.linalg.norm(weights_LIP - oldWeights, 2)\n",
    "        \n",
    "        # print(\"Iteration {} | Error = {}\".format(k+1, ERROR[:, k]))\n",
    "        \n",
    "        if np.isnan(weights_LIP).any == True:\n",
    "            print(\"The weights have diverged\")\n",
    "            break\n",
    "\n",
    "    return uNextFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1999)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data to train and validate\n",
    "# so we need samples from an optimal policy, ideally this is a sample set from an \"expert\" demonstrator\n",
    "# but in less than ideal scenarions we use an optimal policy from an pretrained Q-learning instance\n",
    "# train =  : this is a line where the optimal policy samples are going to be generated or just loaded, ask edgar how to save values\n",
    "# U_true =  : different samples with the which to test the \n",
    "\n",
    "train_states = np.loadtxt('Train_States.txt', dtype=float)\n",
    "train_control = np.loadtxt('Train_Controls.txt', dtype=float)\n",
    "U_true = train_control.reshape(1,train_control.size)\n",
    "X = train_states\n",
    "\n",
    "policy = np.concatenate((X, U_true))\n",
    "train_og = tf.reshape(policy, (policy.shape))\n",
    "policy = policy.reshape(1,policy.shape[0],policy.shape[1])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Gradient_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " x_in (InputLayer)           [(None, 3, 1999)]         0         \n",
      "                                                                 \n",
      " denseC1 (Dense)             (None, 3, 4)              8000      \n",
      "                                                                 \n",
      " denseC2 (Dense)             (None, 3, 5)              25        \n",
      "                                                                 \n",
      " linearC3 (Dense)            (None, 3, 3)              18        \n",
      "                                                                 \n",
      " aOut (Dense)                (None, 3, 1)              4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,047\n",
      "Trainable params: 8,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent network\n",
    "# Edit the network should have the dempostrator policy as inputs, and output the R and Q\n",
    "input_shape = numberOfControls + numberOfStates\n",
    "output_shape = input_shape\n",
    "\n",
    "model_in = Input(shape=(train_og.shape), name=\"x_in\")\n",
    "z = Dense(4, activation='relu', name=\"denseC1\")(model_in)\n",
    "z = Dense(5, activation='relu', name=\"denseC2\")(z)\n",
    "z = Dense(3, activation='linear', name=\"linearC3\")(z)\n",
    "out = Dense(1, name=\"aOut\")(z)\n",
    "\n",
    "model_GD = Model(inputs=[model_in], outputs=[out], name=\"Gradient_Model\")\n",
    "#model_GD.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "model_GD.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 3, 1999) for input KerasTensor(type_spec=TensorSpec(shape=(None, 3, 1999), dtype=tf.float32, name='x_in'), name='x_in', description=\"created by layer 'x_in'\"), but it was called on an input with incompatible shape (None, 3).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"Gradient_Model\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"denseC1\" is incompatible with the layer: expected axis -1 of input shape to have value 1999, but received input with shape (None, 3)\n    \n    Call arguments received by layer \"Gradient_Model\" \"                 f\"(type Functional):\n      • inputs=tf.Tensor(shape=(None, 3), dtype=int32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9860\\1842215867.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mguess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_GD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"Gradient_Model\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"denseC1\" is incompatible with the layer: expected axis -1 of input shape to have value 1999, but received input with shape (None, 3)\n    \n    Call arguments received by layer \"Gradient_Model\" \"                 f\"(type Functional):\n      • inputs=tf.Tensor(shape=(None, 3), dtype=int32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "guess = np.array((1,2,4)).reshape(1,input_shape)\n",
    "print(guess.shape)\n",
    "prediction = model_GD.predict(guess)\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 3.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.array((1,2,3))\n",
    "np.eye(n.size) * n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(network_output):\n",
    "    network_output = np.array(network_output).reshape(3,1)\n",
    "    Qs = np.eye(numberOfStates)\n",
    "    Rs = np.eye(numberOfControls)\n",
    "\n",
    "    Qss = np.zeros((1, numberOfStates))\n",
    "    Rss = np.zeros((1, numberOfControls))\n",
    "\n",
    "    for i in range(numberOfStates):\n",
    "        Qss[:,i] = network_output[i]\n",
    "    for j in range(numberOfControls):\n",
    "        Rss[:,j] = network_output[numberOfStates+j]\n",
    "\n",
    "    print(Qss)\n",
    "\n",
    "    Q = Qs * Qss\n",
    "    R = Rs * Rss\n",
    "\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'trainable_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13912\\1513136899.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#separate(prediction.T)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'trainable_variables'"
     ]
    }
   ],
   "source": [
    "\n",
    "#separate(prediction.T)\n",
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25915238 -0.09271409]]\n",
      "[None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['denseC1/kernel:0', 'denseC1/bias:0', 'denseC2/kernel:0', 'denseC2/bias:0', 'linearC3/kernel:0', 'linearC3/bias:0', 'aOut/kernel:0', 'aOut/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'denseC1/kernel:0' shape=(1999, 4) dtype=float32, numpy=\narray([[ 0.0091089 , -0.04871399,  0.00050838,  0.00884101],\n       [-0.03221896,  0.02565037, -0.01074677,  0.03093069],\n       [ 0.04772449,  0.02825473,  0.04298745, -0.03740874],\n       ...,\n       [-0.04829378, -0.04537277,  0.00629076, -0.04414977],\n       [-0.04932998,  0.00678224, -0.0043845 ,  0.01002862],\n       [-0.02386786, -0.01392929,  0.01743731, -0.01504624]],\n      dtype=float32)>), (None, <tf.Variable 'denseC1/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'denseC2/kernel:0' shape=(4, 5) dtype=float32, numpy=\narray([[ 0.5807724 , -0.00480539,  0.11797154, -0.4891484 , -0.75998527],\n       [ 0.6021545 , -0.48518535,  0.56726086, -0.5671906 ,  0.32049298],\n       [-0.21754128,  0.59352934,  0.08948421,  0.7719984 ,  0.67068076],\n       [ 0.7549732 , -0.1191321 ,  0.47170222,  0.5969547 , -0.4807818 ]],\n      dtype=float32)>), (None, <tf.Variable 'denseC2/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'linearC3/kernel:0' shape=(5, 3) dtype=float32, numpy=\narray([[ 0.5497882 ,  0.8140982 , -0.84831095],\n       [ 0.38342136, -0.48034763, -0.11172938],\n       [ 0.68278164, -0.85856414, -0.4141432 ],\n       [ 0.51267976,  0.4443633 ,  0.0804984 ],\n       [-0.6962507 , -0.82539636, -0.31413323]], dtype=float32)>), (None, <tf.Variable 'linearC3/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'aOut/kernel:0' shape=(3, 1) dtype=float32, numpy=\narray([[-1.0690477 ],\n       [-0.7712507 ],\n       [-0.48756385]], dtype=float32)>), (None, <tf.Variable 'aOut/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9860\\1548029067.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_GD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_GD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0macc_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    687\u001b[0m           \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \"\"\"\n\u001b[1;32m--> 689\u001b[1;33m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\uhh443\\Miniconda3\\envs\\Py\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[1;34mf\"No gradients provided for any variable: {variable}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;34mf\"Provided `grads_and_vars` is {grads_and_vars}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['denseC1/kernel:0', 'denseC1/bias:0', 'denseC2/kernel:0', 'denseC2/bias:0', 'linearC3/kernel:0', 'linearC3/bias:0', 'aOut/kernel:0', 'aOut/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'denseC1/kernel:0' shape=(1999, 4) dtype=float32, numpy=\narray([[ 0.0091089 , -0.04871399,  0.00050838,  0.00884101],\n       [-0.03221896,  0.02565037, -0.01074677,  0.03093069],\n       [ 0.04772449,  0.02825473,  0.04298745, -0.03740874],\n       ...,\n       [-0.04829378, -0.04537277,  0.00629076, -0.04414977],\n       [-0.04932998,  0.00678224, -0.0043845 ,  0.01002862],\n       [-0.02386786, -0.01392929,  0.01743731, -0.01504624]],\n      dtype=float32)>), (None, <tf.Variable 'denseC1/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'denseC2/kernel:0' shape=(4, 5) dtype=float32, numpy=\narray([[ 0.5807724 , -0.00480539,  0.11797154, -0.4891484 , -0.75998527],\n       [ 0.6021545 , -0.48518535,  0.56726086, -0.5671906 ,  0.32049298],\n       [-0.21754128,  0.59352934,  0.08948421,  0.7719984 ,  0.67068076],\n       [ 0.7549732 , -0.1191321 ,  0.47170222,  0.5969547 , -0.4807818 ]],\n      dtype=float32)>), (None, <tf.Variable 'denseC2/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'linearC3/kernel:0' shape=(5, 3) dtype=float32, numpy=\narray([[ 0.5497882 ,  0.8140982 , -0.84831095],\n       [ 0.38342136, -0.48034763, -0.11172938],\n       [ 0.68278164, -0.85856414, -0.4141432 ],\n       [ 0.51267976,  0.4443633 ,  0.0804984 ],\n       [-0.6962507 , -0.82539636, -0.31413323]], dtype=float32)>), (None, <tf.Variable 'linearC3/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'aOut/kernel:0' shape=(3, 1) dtype=float32, numpy=\narray([[-1.0690477 ],\n       [-0.7712507 ],\n       [-0.48756385]], dtype=float32)>), (None, <tf.Variable 'aOut/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "for epoch in range(numberOfEpochs):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # The model will predict that the reward function parameters\n",
    "        \n",
    "        RQ_predict = model_GD(policy)   # train variable refers to the x,u values that the network uses to predict RQ: this are the values for the \n",
    "        \n",
    "        Qt, Rt = separate(RQ_predict)\n",
    "        uNextFunction = Q_Learn(Rt, Qt, weights_LIP)\n",
    "\n",
    "        # Using the simulation, add a line using this controls to see if the simulation matches expected outcome, or just have a previously simulated data set from where to compare (better idea)(u_true)\n",
    "        # The X[] need to be changed to accommodate the testing\n",
    "        # U_true is the true value, so the values of the \"actions\" that are recorded from the simulations? How would it work? U_true needs to be a pre-labeled data set \n",
    "        U_predict = np.array([uNextFunction(X[0,:], X[1,:])])\n",
    "        loss = loss_train(U_true, U_predict)\n",
    "    \n",
    "\n",
    "    gradients = tape.gradient(loss, model_GD.trainable_variables)\n",
    "    print(gradients)\n",
    "    optimizer.apply_gradients(zip(gradients, model_GD.trainable_variables))\n",
    "    acc_metric.update_state(U_true, U_predict)\n",
    "\n",
    "    train_acc = acc_metric.result()\n",
    "    print(f\"Accuraty over epoch {train_acc}\")\n",
    "    acc_metric.reset_states()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test would look similar to the QLIP_Single Network\n",
    "# Something like this, vareiables need to be changed to fit this script\n",
    "\n",
    "X = np.zeros((numberOfStates, maxQuantizedTime))\n",
    "X[:, 0] = np.array([[1.], [1.], [1.]]).reshape(numberOfStates,)\n",
    "\n",
    "U = np.zeros((numberOfControls, maxQuantizedTime-1))\n",
    "\n",
    "for t in range(maxQuantizedTime-1):\n",
    "    \n",
    "    uOn = G(cs(X[:,t]))\n",
    "\n",
    "    U[:, t] = np.array([u1NextFunction(X[0,t], X[1,t], X[2,t]), u2NextFunction(X[0,t], X[1,t], X[2,t]), u3NextFunction(X[0,t], X[1,t], X[2,t])]).reshape((numberOfControls,))\n",
    "    \n",
    "    X[:, t+1] = (F(cs(X[:,t])) + (uOn[:,0,:] * U[0,t] + uOn[:,1,:] * U[1,t] + uOn[:,2,:] * U[2,t]).T).reshape((numberOfStates,))\n",
    "    \n",
    "\n",
    "Time = np.linspace(0, maxQuantizedTime, maxQuantizedTime) * deltaT\n",
    "timeControl = np.linspace(0, maxQuantizedTime, maxQuantizedTime-1) * deltaT\n",
    "iterationX = np.linspace(0, maxLooping, maxLooping)\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.title('States')\n",
    "plt.plot(Time, X[0,:])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.grid(visible=True)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(Time, X[1,:])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.grid(visible=True)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(Time, X[2,:])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('$x_3$')\n",
    "plt.grid(visible=True)\n",
    "\n",
    "plt.figure(num=2)\n",
    "plt.subplot(3,1,1)\n",
    "plt.title('Control')\n",
    "plt.plot(timeControl, U[0,:])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('[$u_1$]')\n",
    "plt.grid(visible=True)\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(timeControl, U[1,:])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('[$u_2$]')\n",
    "plt.grid(visible=True)\n",
    "\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(timeControl, U[2,:])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('[$u_3$]')\n",
    "plt.grid(visible=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(num=3)\n",
    "plt.title('Weights')\n",
    "for l in range(numberOfNeurons):\n",
    "    plt.plot(iterationX, weightHistory[l,:])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Weights')\n",
    "plt.grid(visible=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(num=4)\n",
    "plt.title('Error')\n",
    "plt.plot(iterationX, ERROR[0,:])\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(visible=True)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b95a446d0db681a9f24dd77b6db59163d5fb76aa220ebc79ffa90fec0ea8bff3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
